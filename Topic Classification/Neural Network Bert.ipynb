{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5d24203-6f00-4e12-9fd9-5b450dfca26a",
   "metadata": {},
   "source": [
    "<h2 align=center> Fine-Tune BERT for Text Classification with TensorFlow</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e99360-c27b-46b1-bc10-eba88ce28ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "#bert libraries\n",
    "from transformers import TFAutoModel, BertTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27fcc361-e989-48a6-9922-3e18123777fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Pandas Version: {pd.__version__}')\n",
    "print(f'Numpy Version: {np.__version__}')\n",
    "print(f'Seaborn Version: {sns.__version__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be2c64d-5f11-4518-bac6-5baf6722e3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4c8e21-ff65-443e-be1c-1fc8e19bf895",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff9d1c1-12ed-43fd-bf8f-ce23434926ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r'df.csv')\n",
    "df = df[['Keyword','Label']]\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3f5f34-fa85-4e65-9957-c7490f7e92ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_analysis_values(dataframe, columnname):\n",
    "    print(f'Shape of the dataframe is {dataframe.shape}')\n",
    "    print(dataframe.info())\n",
    "    print('Data labels Distribution')\n",
    "    print(dataframe[columnname].value_counts())\n",
    "    print()\n",
    "    fig=plt.figure(figsize=(10, 5))\n",
    "    plt.hist(dataframe[columnname],color = \"skyblue\", lw=0)\n",
    "    plt.xlabel('Types of Labels')\n",
    "    plt.ylabel('Number of Instances')\n",
    "    plt.title('Distribution of Label in Dataset');\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8777ba4b-6700-4aa4-8d52-3ba29eaf4d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_analysis_values(df, 'Label')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b77725-34f1-4735-8aaf-b79221c32bfb",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93916491-5266-4583-8b29-7cc09535c2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def minimum_label_count(dataframe, columnname):\n",
    "    label_count = dataframe[columnname].value_counts()\n",
    "    criteria = label_count < 100\n",
    "    col_out = criteria[criteria == True]\n",
    "    remove_col_name = col_out.index[0]\n",
    "    print(f'Column {remove_col_name} fulfills the criteria < 100. Thus, removing it out of our dataframe')\n",
    "    \n",
    "    dataframe = dataframe[dataframe[columnname] != 'remove_col_name']\n",
    "    print(f'New Shape of the Dataframe : {dataframe.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573d1cb3-c0c1-4fb0-98c0-07a49420534c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "minimum_label_count(df, 'Label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f3e59d-9121-43c3-a0dc-12b156e71e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checks if the data columns are present in the df\n",
    "def check_if_columns_in_data_frame(df, columns_needed):\n",
    "    checked = [col for col in columns_needed if col in df.columns.values]\n",
    "    if len(checked) < len(columns_needed):\n",
    "        missing_cols = [col for col in columns_needed if col not in checked]\n",
    "        raise ValueError(\n",
    "            f\"Column(s) {missing_cols} not found in dataset, found {checked}. Please change your column names\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660bb801-8729-4753-88fb-887eb1f9dc4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _prepare_df(df):\n",
    "    df.columns = map(str.lower, df.columns)\n",
    "    check_if_columns_in_data_frame(df, [\"keyword\", \"label\"])\n",
    "    return df.drop_duplicates(\n",
    "        subset=[\"keyword\"]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4988f6d0-3d5d-4a0c-ac5e-01c7785a6ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "_prepare_df(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9332641d-6744-4c4e-975b-af5365689b05",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2038320-162c-407d-8bc0-cd11472544af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(dataframe, columntoencode, newcolumn):\n",
    "    possible_labels = dataframe[columntoencode].unique()\n",
    "    label_dict = {}\n",
    "    \n",
    "    for index, possible_label in enumerate(possible_labels):\n",
    "        label_dict[possible_label] = index\n",
    "        \n",
    "    dataframe[columntoencode] = dataframe[columntoencode].replace(label_dict)\n",
    "    \n",
    "    dataframe[newcolumn] = dataframe[columntoencode].replace(label_dict)\n",
    "    \n",
    "    dataframe.drop([columntoencode], axis = 1, inplace = True)\n",
    "    return dataframe.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f6d056-c703-4948-9176-c818964c7a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_data(df,'label','Label')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deee4010-12bb-4337-ab3d-3ae4f7456c5f",
   "metadata": {},
   "source": [
    "### Tokenize and Form Input Layers for Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1b49d4-0e2e-4f51-8f70-44529553b58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "engmodel = 'bert-base-cased'\n",
    "dutchmodel = 'GroNLP/bert-base-dutch-cased'\n",
    "\n",
    "seq_len = 128\n",
    "num_samples = len(df)\n",
    "\n",
    "#token ids\n",
    "x_ids = np.zeros((num_samples,seq_len))\n",
    "x_mask = np.zeros((num_samples,seq_len)) #attention mask\n",
    "\n",
    "print(x_ids.shape)\n",
    "print(x_mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843120c5-5d67-4fa6-91c6-c15e6139d690",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initiating the bert tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(engmodel)\n",
    "\n",
    "#going through each of the sentences in keyword and tokenizing them. (forms a dictionary)\n",
    "for i, phrase in enumerate(df['keyword']):\n",
    "    tokens = tokenizer.encode_plus(phrase, max_length = seq_len, truncation=True,\n",
    "                                  padding = 'max_length', add_special_tokens=True,\n",
    "                                  return_tensors='tf')\n",
    "    #filling the arrays of x_ids and x_masks with the new tokenized values \n",
    "    x_ids[i, :] = tokens['input_ids']\n",
    "    x_mask[i, :] = tokens['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4b530b-2614-4eeb-9097-171137724dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b35bf5-7ed5-4bda-8b8f-bb5a031cc5f4",
   "metadata": {},
   "source": [
    "The 101 is the [CLS] tokens and the 0 is the [PAD] tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cbf48f3-4c3d-41d8-accf-4e7901a66932",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0933d1cc-abca-4127-a840-254fbb9615ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#one hot encoding \n",
    "arr = df['Label'].values\n",
    "arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433fe9dd-c6a2-476f-a18c-1223c667a66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize a zero array \n",
    "#columns for each class\n",
    "labels = np.zeros((num_samples, arr.max()+1)) \n",
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a314b46-99bf-426a-b244-b04d53321f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#specify the row \n",
    "labels[np.arange(num_samples), arr] = 1\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17625e6c-8e69-4e24-866c-69861a6613e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#putting them into format that tensorflow will read\n",
    "\n",
    "import tensorflow as tf \n",
    "dataset = tf.data.Dataset.from_tensor_slices((x_ids, x_mask, labels))\n",
    "\n",
    "#shows the very top batch/ sample\n",
    "dataset.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07cafd65-b52f-4619-a08b-1ee0833216bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745b3f4b-8cf1-4ff2-9229-4a4e5641ddde",
   "metadata": {},
   "source": [
    "Each sample in our dataset is a tuple containing single x_ids, x_mask and label tensors. However, when feeding data into our neural network we need a two-item tuple in the format (\\<inputs>, \\<outputs>). Now, we have two tensors for our inputs - so, what we do is enter our \\<inputs> tensor as dictionary. \n",
    " ### {\n",
    "    'input_ids': <input_id_tensor>,\n",
    "    'attention_mask': <mask_tensor>\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41a26c6-a878-425f-9ab6-e69c4a723086",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_func(input_ids, masks, labels):\n",
    "    return {'input_ids': input_ids, 'attention_mask': masks}, labels # we convert our three-item tuple into a two-item tuple where the input item is a dictionary\n",
    "\n",
    "\n",
    "# then we use the dataset map method to apply this transformation\n",
    "dataset = dataset.map(map_func)\n",
    "\n",
    "dataset.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245ba738-8f9b-427e-a2cb-cff8f79ed74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "#for shuffling the data values\n",
    "#drop remainder is making sure the batches are 32 and \n",
    "dataset = dataset.shuffle(10000).batch(batch_size, drop_remainder=True)\n",
    "\n",
    "dataset.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3a4698-9abe-4344-a642-f974a63a6f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0250a0-0f0b-4bcc-88fa-429c73c6f4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "split = 0.2\n",
    "\n",
    "           #number of samples\n",
    "steps_per_epoch = int((x_ids.shape[0] / batch_size) * split)\n",
    "print(steps_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9290476c-0d57-410d-808f-3fe03d2e28a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fitting the size of the dataset values in the training set\n",
    "train_ds = dataset.take(steps_per_epoch)\n",
    "print(len(train_ds))\n",
    "\n",
    "#fitting the remaining by skipping the trained samples for the validation set\n",
    "val_ds = dataset.skip(steps_per_epoch)\n",
    "print(len(val_ds))\n",
    "\n",
    "del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f862090-f253-45a6-ad57-41372a05703f",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"The quick brown fox jumps over the lazy dog.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41fc05ac-2f77-44f8-bba2-acf7a0a43d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_bert = naw.ContextualWordEmbsAug(\n",
    "    model_path='bert-base-uncased', action=ACT, top_k=TOPK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18ac748-2bf9-481a-be2e-5ca491e13189",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_text(df,samples=300):\n",
    "    aug_bert = naw.ContextualWordEmbsAug(\n",
    "    model_path='bert-base-uncased', action=ACT, top_k=TOPK)\n",
    "    new_text=[]\n",
    "    \n",
    "    \n",
    "    ##selecting the minority class samples\n",
    "    df_n=df[df.Label==3].reset_index(drop=True)\n",
    "\n",
    "    ## data augmentation loop\n",
    "    for i in tqdm(np.random.randint(0,len(df_n),samples)):\n",
    "        \n",
    "            text = df_n.iloc[i]['text']\n",
    "            augmented_text = aug_bert(text)\n",
    "            new_text.append(augmented_text)\n",
    "    \n",
    "    \n",
    "    ## dataframe\n",
    "    new=pd.DataFrame({'text':new_text,'target':1})\n",
    "    df=shuffle(df.append(new).reset_index(drop=True))\n",
    "    return df\n",
    "   \n",
    "train_ds = augment_text(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2568ec-13ba-4ec8-be77-11458082e949",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert = TFAutoModel.from_pretrained(engmodel)\n",
    "\n",
    "bert.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d9e9bd-3cb6-4d84-90db-584a7a072b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_classifier_model():\n",
    "    \n",
    "    #Input Layers\n",
    "    input_ids = tf.keras.layers.Input(shape=(seq_len,), name = 'input_ids', dtype = 'int32')\n",
    "    mask_ids = tf.keras.layers.Input(shape=(seq_len,), name = 'attention_mask', dtype = 'int32')\n",
    "\n",
    "    #Creating embeddings from bert \n",
    "    #[0] is the 3d tensors pooled into 2d tensors and we have dense layers so we need the pooled layer\n",
    "    embeddings = bert.bert(input_ids,attention_mask= mask_ids)[1]\n",
    "\n",
    "    #Convert these embeddings into our label predictions\n",
    "    #passing the embeddings into the dense layer\n",
    "    x = tf.keras.layers.Dense(1024, activation = 'relu')(embeddings)\n",
    "    y = tf.keras.layers.Dense(arr.max()+1, activation='softmax', name = 'outputs')(x)\n",
    "    \n",
    "    model = tf.keras.Model(inputs=[input_ids, mask_ids], outputs = y)\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate = 5e-5, decay = 1e-6)\n",
    "    loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "    acc = tf.keras.metrics.CategoricalAccuracy('accuracy')\n",
    "    \n",
    "    model.compile(optimizer=optimizer, loss = loss, metrics=[acc])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8080605-6918-427e-b578-8c1846a707c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_classifier_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4367ec0-02ae-4c3c-a9b6-d6c0dff22343",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#model.compile(optimizer=optimizer, loss = loss, metrics=[acc])\n",
    "history = model.fit(train_ds,validation_data=val_ds,\n",
    "                    epochs = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde6e303-f343-4df2-86fb-03b54b480a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model Accuracy \n",
    "plt.figure(figsize=(9,5))\n",
    "plt.plot(history.history['accuracy'], color = 'blueviolet', marker = 'h', label = 'Train Set')\n",
    "plt.plot(history.history['val_accuracy'], color = 'lightcoral', marker = 'd', label = 'Test Set')\n",
    "\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Number of Epochs')\n",
    "plt.ylabel('Accuracy Score')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e08dce-660c-430f-a9e8-d7c3d905cd49",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model Loss \n",
    "plt.figure(figsize=(9,5))\n",
    "plt.plot(history.history['loss'], color = 'blueviolet', marker = 'h', label = 'Train Set')\n",
    "plt.plot(history.history['val_loss'], color = 'lightcoral', marker = 'd', label = 'Test Set')\n",
    "\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Number of Epochs')\n",
    "plt.ylabel('Loss Score')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758ab8f9-c069-416a-8bb9-7766dd867968",
   "metadata": {},
   "source": [
    "## Model Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592ac0b4-9523-47ad-b4c5-f90163a6516f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss, train_acc = model.evaluate(train_ds)\n",
    "test_loss, test_acc = model.evaluate(val_ds)\n",
    "print(\"Training Set Accuracy: {:.2f}\" . format(train_acc))\n",
    "print(\"Test Set Accuracy: {:.2f}\" . format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606022d6-a28d-4447-bc37-90cb9ca7eb6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(y_pred, y_true):\n",
    "    y_pred = model.predict(val_ds)\n",
    "    y_pred = tf.argmax(y_pred, axis=1) #axis = 1, to get the highest common values and for classfication metrics to handle a multiclass and continou-outpuits targers.\n",
    "    \n",
    "    y_true = tf.concat([y for x, y in val_ds], axis=0)\n",
    "    y_true = np.argmax(y_true, axis = 1)\n",
    "    \n",
    "    \n",
    "    print(len(y_pred))\n",
    "    print(len(y_true))\n",
    "    \n",
    "    #Plot Confusion Matrix\n",
    "    sns.heatmap(confusion_matrix(y_true,y_pred), annot = True, cmap = plt.cm.Blues, fmt = \".1f\")\n",
    "    plt.title(\"Confusion Matrix of Test Data\")\n",
    "    plt.xlabel(\"Predicted Label\")\n",
    "    plt.ylabel(\"True Label\")\n",
    "    plt.show()\n",
    "    \n",
    "    #Plot Classification Report\n",
    "    print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a097dec2-43d0-461b-9528-4d0bc7fdaf9b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.save('label_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b5dd63-9419-4282-a36d-4bec90927b08",
   "metadata": {},
   "source": [
    "### Prediction on New Keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6f8c26-f48a-492e-bbca-e22ec320e2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9efbf474-0617-491c-9014-c6fe31316a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize tokenizer from transformers\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "\n",
    "def prep_data(text):\n",
    "    # tokenize to get input IDs and attention mask tensors\n",
    "    tokens = tokenizer.encode_plus(text, \n",
    "                                   max_length=128,\n",
    "                                   truncation=True, \n",
    "                                   padding='max_length',\n",
    "                                   add_special_tokens=True, \n",
    "                                   return_token_type_ids=False,\n",
    "                                   return_tensors='tf')\n",
    "    # tokenizer returns int32 tensors, we need to return float64, so we use tf.cast\n",
    "    return {'input_ids': tensorflow.cast(tokens['input_ids'], tensorflow.float64),\n",
    "            'attention_mask': tensorflow.cast(tokens['attention_mask'], tensorflow.float64)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c85b818-74ed-4f0d-ad95-69bda39acafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword = prep_data(\"best camera for me\")\n",
    "label = model.predict(keyword)[0]\n",
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a413789-2030-4b03-88d1-9572fc9d55cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.argmax(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83de018f-0857-4337-99cd-7518ff8662df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cpu.m80",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m80"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
