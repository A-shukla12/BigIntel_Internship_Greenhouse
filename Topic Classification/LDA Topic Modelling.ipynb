{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed610969-2239-41e6-b71c-822e4686c3f4",
   "metadata": {},
   "source": [
    "<h2 align=center> Topic Modelling using Latent Dirichlet Allocation</h2>\n",
    "\n",
    "This notebook contains the approach for topic modelling for the BigIntel project by using LDA i.e. Latent Dirichlet Allocation technique. LDA's approach to topic modelling is it considers each document as a collection of topics in a certain proportion. And each topic as a collection of keywords, again, in a certain proportion. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2587a1-417d-43e5-8ed9-2c1e1422787f",
   "metadata": {},
   "source": [
    "We can describe the generative process of LDA as, given the M number of documents, N number of words, and prior K number of topics, the model trains to output:\n",
    "\n",
    "- psi, the distribution of words for each topic K\n",
    "- phi, the distribution of topics for each document i"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a27aa98-eeec-4700-8dfa-f65f5e790c68",
   "metadata": {},
   "source": [
    "Parameters of LDA\n",
    "- Alpha parameter is Dirichlet prior concentration parameter that represents document-topic density — with a higher alpha, documents are assumed to be made up of more topics and result in more specific topic distribution per document.\n",
    "- Beta parameter is the same prior concentration parameter that represents topic-word density — with high beta, topics are assumed to made of up most of the words and result in a more specific word distribution per topic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf3f301-7bec-4b1e-88f6-473c28e9146e",
   "metadata": {},
   "source": [
    "This project/notebook consists of several Tasks.\n",
    "\n",
    "- **[Task 1]()**: Installing all dependencies for our environment.\n",
    "- **[Task 2]()**: Importing the required libraries in the environment.\n",
    "- **[Task 3]()**: Exploratory Data Analysis\n",
    "- **[Task 4]()**: Data Analysis and Pre-processing of keywords by one-hot-encoding.\n",
    "- **[Task 5]()**: Data Preprocessing\n",
    "- **[Task 6]()**: Creating the Dictionary and Corpus needed for Topic Modelling\n",
    "- **[Task 7]()**: Building the Topic Model\n",
    "- **[Task 8]()**: Analysis top n keywords in each topic "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23e943c-bcb4-4528-8f10-801e73af0269",
   "metadata": {},
   "source": [
    "### Task 1: Installing all dependencies for our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8dbc9c41-23f1-49be-80cf-e22e81008309",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied (use --upgrade to upgrade): spacy in /opt/conda/lib/python3.7/site-packages\n",
      "Requirement already satisfied (use --upgrade to upgrade): thinc<8.1.0,>=8.0.9 in /opt/conda/lib/python3.7/site-packages (from spacy)\n",
      "Requirement already satisfied (use --upgrade to upgrade): typing-extensions<4.0.0.0,>=3.7.4 in /opt/conda/lib/python3.7/site-packages (from spacy)\n",
      "Requirement already satisfied (use --upgrade to upgrade): jinja2 in /opt/conda/lib/python3.7/site-packages (from spacy)\n",
      "Requirement already satisfied (use --upgrade to upgrade): pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /opt/conda/lib/python3.7/site-packages (from spacy)\n",
      "Requirement already satisfied (use --upgrade to upgrade): catalogue<2.1.0,>=2.0.6 in /opt/conda/lib/python3.7/site-packages (from spacy)\n",
      "Requirement already satisfied (use --upgrade to upgrade): wasabi<1.1.0,>=0.8.1 in /opt/conda/lib/python3.7/site-packages (from spacy)\n",
      "Requirement already satisfied (use --upgrade to upgrade): requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.7/site-packages (from spacy)\n",
      "Requirement already satisfied (use --upgrade to upgrade): blis<0.8.0,>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from spacy)\n",
      "Requirement already satisfied (use --upgrade to upgrade): cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy)\n",
      "Requirement already satisfied (use --upgrade to upgrade): spacy-legacy<3.1.0,>=3.0.8 in /opt/conda/lib/python3.7/site-packages (from spacy)\n",
      "Requirement already satisfied (use --upgrade to upgrade): preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy)\n",
      "Requirement already satisfied (use --upgrade to upgrade): tqdm<5.0.0,>=4.38.0 in /opt/conda/lib/python3.7/site-packages (from spacy)\n",
      "\u001b[31mException:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pip/_vendor/pkg_resources/__init__.py\", line 2851, in _dep_map\n",
      "    return self.__dep_map\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pip/_vendor/pkg_resources/__init__.py\", line 2685, in __getattr__\n",
      "    raise AttributeError(attr)\n",
      "AttributeError: _DistInfoDistribution__dep_map\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pip/basecommand.py\", line 209, in main\n",
      "    status = self.run(options, args)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pip/commands/install.py\", line 310, in run\n",
      "    wb.build(autobuilding=True)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pip/wheel.py\", line 748, in build\n",
      "    self.requirement_set.prepare_files(self.finder)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pip/req/req_set.py\", line 360, in prepare_files\n",
      "    ignore_dependencies=self.ignore_dependencies))\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pip/req/req_set.py\", line 647, in _prepare_file\n",
      "    set(req_to_install.extras) - set(dist.extras)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pip/_vendor/pkg_resources/__init__.py\", line 2810, in extras\n",
      "    return [dep for dep in self._dep_map if dep]\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pip/_vendor/pkg_resources/__init__.py\", line 2853, in _dep_map\n",
      "    self.__dep_map = self._compute_dependencies()\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pip/_vendor/pkg_resources/__init__.py\", line 2886, in _compute_dependencies\n",
      "    common = frozenset(reqs_for_extra(None))\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pip/_vendor/pkg_resources/__init__.py\", line 2883, in reqs_for_extra\n",
      "    if req.marker_fn(override={'extra':extra}):\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pip/_vendor/_markerlib/markers.py\", line 113, in marker_fn\n",
      "    return eval(compiled_marker, environment)\n",
      "  File \"<environment marker>\", line 1, in <module>\n",
      "NameError: name 'platform_system' is not defined\u001b[0m\n",
      "\u001b[33mYou are using pip version 8.1.1, however version 21.3.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "2021-12-13 10:57:28.378578: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory\n",
      "2021-12-13 10:57:28.378638: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "\u001b[38;5;3m⚠ As of spaCy v3.0, shortcuts like 'en' are deprecated. Please use the\n",
      "full pipeline package name 'en_core_web_sm' instead.\u001b[0m\n",
      "Requirement already satisfied (use --upgrade to upgrade): en-core-web-sm==3.1.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.1.0/en_core_web_sm-3.1.0-py3-none-any.whl#egg=en_core_web_sm==3.1.0 in /opt/conda/lib/python3.7/site-packages\n",
      "Requirement already satisfied (use --upgrade to upgrade): spacy<3.2.0,>=3.1.0 in /opt/conda/lib/python3.7/site-packages (from en-core-web-sm==3.1.0)\n",
      "Requirement already satisfied (use --upgrade to upgrade): typing-extensions<4.0.0.0,>=3.7.4 in /opt/conda/lib/python3.7/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0)\n",
      "Requirement already satisfied (use --upgrade to upgrade): typer<0.5.0,>=0.3.0 in /opt/conda/lib/python3.7/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0)\n",
      "Requirement already satisfied (use --upgrade to upgrade): pathy>=0.3.5 in /opt/conda/lib/python3.7/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0)\n",
      "Requirement already satisfied (use --upgrade to upgrade): tqdm<5.0.0,>=4.38.0 in /opt/conda/lib/python3.7/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0)\n",
      "\u001b[31mException:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pip/_vendor/pkg_resources/__init__.py\", line 2851, in _dep_map\n",
      "    return self.__dep_map\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pip/_vendor/pkg_resources/__init__.py\", line 2685, in __getattr__\n",
      "    raise AttributeError(attr)\n",
      "AttributeError: _DistInfoDistribution__dep_map\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pip/basecommand.py\", line 209, in main\n",
      "    status = self.run(options, args)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pip/commands/install.py\", line 310, in run\n",
      "    wb.build(autobuilding=True)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pip/wheel.py\", line 748, in build\n",
      "    self.requirement_set.prepare_files(self.finder)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pip/req/req_set.py\", line 360, in prepare_files\n",
      "    ignore_dependencies=self.ignore_dependencies))\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pip/req/req_set.py\", line 647, in _prepare_file\n",
      "    set(req_to_install.extras) - set(dist.extras)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pip/_vendor/pkg_resources/__init__.py\", line 2810, in extras\n",
      "    return [dep for dep in self._dep_map if dep]\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pip/_vendor/pkg_resources/__init__.py\", line 2853, in _dep_map\n",
      "    self.__dep_map = self._compute_dependencies()\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pip/_vendor/pkg_resources/__init__.py\", line 2886, in _compute_dependencies\n",
      "    common = frozenset(reqs_for_extra(None))\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pip/_vendor/pkg_resources/__init__.py\", line 2883, in reqs_for_extra\n",
      "    if req.marker_fn(override={'extra':extra}):\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pip/_vendor/_markerlib/markers.py\", line 113, in marker_fn\n",
      "    return eval(compiled_marker, environment)\n",
      "  File \"<environment marker>\", line 1, in <module>\n",
      "NameError: name 'platform_system' is not defined\u001b[0m\n",
      "\u001b[33mYou are using pip version 8.1.1, however version 21.3.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install spacy\n",
    "!{sys.executable} -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8e90f23-ade2-4dbd-ae47-adb3b5a3af27",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied (use --upgrade to upgrade): gensim in /opt/conda/lib/python3.7/site-packages\n",
      "Requirement already satisfied (use --upgrade to upgrade): scipy>=0.18.1 in /opt/conda/lib/python3.7/site-packages (from gensim)\n",
      "Requirement already satisfied (use --upgrade to upgrade): six>=1.5.0 in /opt/conda/lib/python3.7/site-packages (from gensim)\n",
      "Requirement already satisfied (use --upgrade to upgrade): smart-open>=1.8.1 in /opt/conda/lib/python3.7/site-packages (from gensim)\n",
      "Requirement already satisfied (use --upgrade to upgrade): numpy>=1.11.3 in /opt/conda/lib/python3.7/site-packages (from gensim)\n",
      "\u001b[33mYou are using pip version 8.1.1, however version 21.3.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09d6c749-d7a4-4ef4-b60a-282523e3883c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied (use --upgrade to upgrade): bokeh in /opt/conda/lib/python3.7/site-packages\n",
      "Requirement already satisfied (use --upgrade to upgrade): packaging>=16.8 in /opt/conda/lib/python3.7/site-packages (from bokeh)\n",
      "Requirement already satisfied (use --upgrade to upgrade): typing-extensions>=3.10.0 in /opt/conda/lib/python3.7/site-packages (from bokeh)\n",
      "Requirement already satisfied (use --upgrade to upgrade): numpy>=1.11.3 in /opt/conda/lib/python3.7/site-packages (from bokeh)\n",
      "Requirement already satisfied (use --upgrade to upgrade): PyYAML>=3.10 in /opt/conda/lib/python3.7/site-packages (from bokeh)\n",
      "Requirement already satisfied (use --upgrade to upgrade): tornado>=5.1 in /opt/conda/lib/python3.7/site-packages (from bokeh)\n",
      "Requirement already satisfied (use --upgrade to upgrade): pillow>=7.1.0 in /opt/conda/lib/python3.7/site-packages (from bokeh)\n",
      "Requirement already satisfied (use --upgrade to upgrade): Jinja2>=2.9 in /opt/conda/lib/python3.7/site-packages (from bokeh)\n",
      "Requirement already satisfied (use --upgrade to upgrade): pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=16.8->bokeh)\n",
      "Requirement already satisfied (use --upgrade to upgrade): MarkupSafe>=2.0 in /opt/conda/lib/python3.7/site-packages (from Jinja2>=2.9->bokeh)\n",
      "\u001b[33mYou are using pip version 8.1.1, however version 21.3.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install bokeh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02eb108d-f4d9-4621-8848-4eb712bc5834",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied (use --upgrade to upgrade): pyLDAvis in /opt/conda/lib/python3.7/site-packages\n",
      "Requirement already satisfied (use --upgrade to upgrade): scikit-learn in /opt/conda/lib/python3.7/site-packages (from pyLDAvis)\n",
      "Requirement already satisfied (use --upgrade to upgrade): funcy in /opt/conda/lib/python3.7/site-packages (from pyLDAvis)\n",
      "Requirement already satisfied (use --upgrade to upgrade): scipy in /opt/conda/lib/python3.7/site-packages (from pyLDAvis)\n",
      "Requirement already satisfied (use --upgrade to upgrade): gensim in /opt/conda/lib/python3.7/site-packages (from pyLDAvis)\n",
      "Requirement already satisfied (use --upgrade to upgrade): numexpr in /opt/conda/lib/python3.7/site-packages (from pyLDAvis)\n",
      "Requirement already satisfied (use --upgrade to upgrade): sklearn in /opt/conda/lib/python3.7/site-packages (from pyLDAvis)\n",
      "Requirement already satisfied (use --upgrade to upgrade): joblib in /opt/conda/lib/python3.7/site-packages (from pyLDAvis)\n",
      "Requirement already satisfied (use --upgrade to upgrade): pandas>=1.2.0 in /opt/conda/lib/python3.7/site-packages (from pyLDAvis)\n",
      "Requirement already satisfied (use --upgrade to upgrade): future in /opt/conda/lib/python3.7/site-packages (from pyLDAvis)\n",
      "Requirement already satisfied (use --upgrade to upgrade): numpy>=1.20.0 in /opt/conda/lib/python3.7/site-packages (from pyLDAvis)\n",
      "Requirement already satisfied (use --upgrade to upgrade): setuptools in /opt/conda/lib/python3.7/site-packages (from pyLDAvis)\n",
      "Requirement already satisfied (use --upgrade to upgrade): jinja2 in /opt/conda/lib/python3.7/site-packages (from pyLDAvis)\n",
      "Requirement already satisfied (use --upgrade to upgrade): threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->pyLDAvis)\n",
      "Requirement already satisfied (use --upgrade to upgrade): six>=1.5.0 in /opt/conda/lib/python3.7/site-packages (from gensim->pyLDAvis)\n",
      "Requirement already satisfied (use --upgrade to upgrade): smart-open>=1.8.1 in /opt/conda/lib/python3.7/site-packages (from gensim->pyLDAvis)\n",
      "Requirement already satisfied (use --upgrade to upgrade): python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas>=1.2.0->pyLDAvis)\n",
      "Requirement already satisfied (use --upgrade to upgrade): pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas>=1.2.0->pyLDAvis)\n",
      "Requirement already satisfied (use --upgrade to upgrade): MarkupSafe>=2.0 in /opt/conda/lib/python3.7/site-packages (from jinja2->pyLDAvis)\n",
      "\u001b[33mYou are using pip version 8.1.1, however version 21.3.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyLDAvis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f245a3a-c13b-4ce1-814e-c6200351d055",
   "metadata": {},
   "source": [
    "### Task 2: Importing the required libraries in the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ccfdb16c-16f6-4ee4-ba9d-4fc56fa55e8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-13 10:57:40.985104: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory\n",
      "2021-12-13 10:57:40.985204: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "#Importing the necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "#tqdm module helps to create progress bars to track how long your code is taking to process\n",
    "from tqdm import tqdm\n",
    "#pprint is to make our topics formatted a little nicer when we take a look\n",
    "from pprint import pprint\n",
    "from collections import Counter\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords \n",
    "\n",
    "#gensim \n",
    "import gensim \n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel, LdaModel\n",
    "\n",
    "#spacy for lemmatization \n",
    "import spacy \n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import scipy.stats as stats\n",
    "\n",
    "#For plotting clustering graph \n",
    "from bokeh.plotting import figure, output_file, show\n",
    "from bokeh.models import Label\n",
    "\n",
    "#LDA and LSA\\\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "#plotting tools \n",
    "#import pyLDAvis\n",
    "import pyLDAvis.gensim_models \n",
    "pyLDAvis.enable_notebook()\n",
    "#import pyLDAvis.gensim\n",
    "\n",
    "\n",
    "# Enable logging for gensim - optional\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\", category = DeprecationWarning) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc9cfa8-ea44-42bc-b1cb-e9ffea9a62c3",
   "metadata": {},
   "source": [
    " \n",
    " * (t-SNE) t-Distributed Stochastic Neighbor Embedding is a non-linear dimensionality reduction algorithm used for exploring high-dimensional data. It maps multi-dimensional data to two or more dimensions suitable for human observation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02cd7073-e698-40ba-990d-844886a54533",
   "metadata": {},
   "source": [
    "### Task 3: Exploratory Data Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76da70b1-4230-44cf-957b-56334a3250ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "unidf = pd.read_csv(r'df.csv',delimiter = '\\t')\n",
    "unidf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d521f3af-fb42-4a57-a3b3-2cf6b7b46633",
   "metadata": {},
   "outputs": [],
   "source": [
    "unidf.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff6604a-fa29-4e32-b4b8-3380e5b74327",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define helper functions\n",
    "def get_top_n_words(n_top_words, count_vectorizer, text_data):\n",
    "    '''\n",
    "    returns a tuple of the top n words in a sample and their \n",
    "    accompanying counts, given a CountVectorizer object and text sample\n",
    "    '''\n",
    "    vectorized_headlines = count_vectorizer.fit_transform(text_data.values)\n",
    "    vectorized_total = np.sum(vectorized_headlines, axis=0)\n",
    "    word_indices = np.flip(np.argsort(vectorized_total)[0,:], 1)\n",
    "    word_values = np.flip(np.sort(vectorized_total)[0,:],1)\n",
    "    \n",
    "    word_vectors = np.zeros((n_top_words, vectorized_headlines.shape[1]))\n",
    "    for i in range(n_top_words):\n",
    "        word_vectors[i,word_indices[0,i]] = 1\n",
    "\n",
    "    words = [word[0].encode('ascii').decode('utf-8') for \n",
    "             word in count_vectorizer.inverse_transform(word_vectors)]\n",
    "\n",
    "    return (words, word_values[0,:n_top_words].tolist()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256b501f-2c3c-461e-ab08-ee6f96fb2234",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initializing the count vector\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vectorizer = CountVectorizer(stop_words = 'english')\n",
    "keyword, keyword_val = get_top_n_words(n_top_words = 15,\n",
    "                                      count_vectorizer = count_vectorizer, \n",
    "                                      text_data = df.Keyword)\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(10,5))\n",
    "ax.bar(range(len(keyword)), keyword_val)\n",
    "ax.set_xticks(range(len(keyword)))\n",
    "ax.set_xticklabels(keyword, rotation = 'vertical')\n",
    "ax.set_title(\"Top Keywords in the df (excluding the stop words)\")\n",
    "ax.set_xlabel(\"Keywords\")\n",
    "ax.set_ylabel(\"Number of Occurences\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da9e059-8212-4a23-8ece-ead4f43a7b61",
   "metadata": {},
   "source": [
    "\n",
    "Next, we generate the hist of keyword word length, and use part-of-speech tagging to understand the types of keywords used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8764d11c-6e31-41e9-b13f-c45b1cdb422c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initializing the count vector\n",
    "count_vectorizer = CountVectorizer(stop_words = 'english')\n",
    "keyword, keyword_val = get_top_n_words(n_top_words = 14,\n",
    "                                      count_vectorizer = count_vectorizer, \n",
    "                                      text_data = df.pos)\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(10,5))\n",
    "ax.bar(range(len(keyword)), keyword_val)\n",
    "ax.set_xticks(range(len(keyword)))\n",
    "ax.set_xticklabels(keyword, rotation = 'vertical')\n",
    "ax.set_title(\"Part-of-Speech Tagging for Keywords\")\n",
    "ax.set_xlabel(\"Type of Keywords\")\n",
    "ax.set_ylabel(\"Number of Occurences\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7aa34eb-2bac-4cd1-ad37-07496f59e807",
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword_len = []\n",
    "\n",
    "for index, row in unidf.iterrows():\n",
    "    #print(len(row['keyword'].split()))\n",
    "    keyword_len.append(len(row['keyword'].split()))\n",
    "    \n",
    "print(f'Average number of words in the keyword are: {np.mean(keyword_len)}' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ceb830-9524-408e-a7eb-181f4a97ea35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "y = stats.norm.pdf(np.linspace(1,10,50), np.mean(keyword_len), np.std(keyword_len))\n",
    "\n",
    "plt.hist(keyword_len, bins= range(1,10), density = True)\n",
    "plt.plot(np.linspace(0,14,50), y, linewidth = 1)\n",
    "plt.title(\"Keyword length\")\n",
    "plt.xlabel(\"Number of words\")\n",
    "plt.ylabel(\"Probability\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf3c3c0-b941-47e5-ae52-ce3852d3d727",
   "metadata": {},
   "source": [
    "### Task 5: Data Preprocessing\n",
    "\n",
    "We use NLTK's <b> wordnet</b> to find the\n",
    "- meanings of words, synonymns, antonyms and more. \n",
    "- We use <b>WordNetLemmatizer</b> to get the root word. Filter out the stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c478fc-8dd5-47ff-9122-b3d4f0a5aac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aacdc012-f891-429a-af37-c24b664305e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "#increasing the stopword list\n",
    "stop_words.extend(['from','subject','re','edu','use'])\n",
    "print(len(stop_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2ad150-7c83-42b4-9c96-2847f18d5bef",
   "metadata": {},
   "source": [
    "### Data Preparation \n",
    "\n",
    "1. `Lemmatization`: Lemmatization is nothing but converting a word to its root word. For example: the lemma of the word ‘machines’ is ‘machine’. Likewise, ‘walking’ –> ‘walk’, ‘mice’ –> ‘mouse’ and so on.\n",
    "2. `Tokenize and Clean-up`: Tokenizing each sentence into list of words, removing punctuations and unnecessary characters altogether. By using <b> Gensim's simple_preprocess() </b>\n",
    "3. `Bigram and Trigram Models`: Bigrams are two words frequently occuring together in a document (eg, social media, where these two words are more likely to occur together then separately). We want to identify these so we can concatenate them and consider them as one word. We use **Pointwise Mutual Information** score to identify significant bigrams and trigrams to concatenate. We also filter them with the filter (noun/adj) (pos), because these are common structures pointing out noun-type n-grams. This helps LDA model better cluster topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97feb0cc-fab6-435f-b176-4ba6d679a995",
   "metadata": {},
   "outputs": [],
   "source": [
    "#transforming all of the keywords into list \n",
    "data = df.Keyword.values.tolist()\n",
    "\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "data_words = list(sent_to_words(data))\n",
    "\n",
    "print(data_words[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eacda9f3-82e0-47aa-a967-6ccfe09a322d",
   "metadata": {},
   "source": [
    "`models.phrases` automatically detects common phrases - aka the multi-word expressions, word n-gram collocations - from a stream of sentences. \n",
    "   - An N-gram means a sequence of N words. So for example, “Medium blog” is a 2-gram (a bigram), “A Medium blog post” is a 4-gram, and “Write on Medium” is a 3-gram (trigram).\n",
    "1. **min_count**: ignores all words and bigrams with total collected count lower than this. bydefault=5. \n",
    "2. **threshold**: represents a threshold for forming the phrases (higher means fewer phrases) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4b6187-1f18-44c0-991e-c5533c035c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(data_words, min_count=1, threshold=2) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
    "\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "# See trigram example\n",
    "print(trigram_mod[bigram_mod[data_words[0]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a5bc40-c360-4b00-8f39-361d8aa52e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(keywords):\n",
    "    #This will remove stopwords and punctuation.\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in keywords]\n",
    "\n",
    "def make_bigram(keywords):\n",
    "    return [bigram_mod[doc] for doc in keywords]\n",
    "\n",
    "def make_trigrams(keywords):\n",
    "    return [trigram_mod[trigram_mod[doc]] for doc in keywords]\n",
    "\n",
    "#reduces the different forms of the word to it's initial form (runs, ran to run)\n",
    "def lemmatization(keywords, allowed_postags=['NOUN','ADJ','VERB','ADV']):\n",
    "    texts_out = []\n",
    "    for sent in keywords:\n",
    "        doc = nlp(\" \".join(sent))\n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8f6856-5c0e-45f5-b413-a34fbc971457",
   "metadata": {},
   "source": [
    "The default spaCy pipeline is laid out like this: \n",
    "* `Tokenizer`: Breaks the full text into individual tokens.\n",
    "* `Tagger`: Tags each token with the part of speech.\n",
    "* `Parser`: Parses into noun chunks, amongst other things. \n",
    "* `Named Entity Recognizer (NER)`: Labels named entities, like U.S.A."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9692f4d6-70b0-47fe-b33a-010221910056",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove stop words \n",
    "data_words_nostops = remove_stopwords(data_words)\n",
    "\n",
    "#Make Bigrams \n",
    "data_words_bigram = make_bigram(data_words_nostops)\n",
    "\n",
    "#Initializing spacy -en- model, keepng only tagger component \n",
    "nlp = spacy.load('en_core_web_sm', disable =['parser','ner'])\n",
    "\n",
    "#Do lemmatization keeping only noun, adj, vb, adv\n",
    "data_lem = lemmatization(data_words_bigram)\n",
    "\n",
    "print(data_lem[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729324ba-1766-4c33-a19b-bccc2feba4af",
   "metadata": {},
   "source": [
    "### Task 6: Creating the Dictionary and Corpus needed for Topic Modelling\n",
    "\n",
    "Gensim creates a unique_id for each word in the document. The produced corpus shown above is the mapping of (word_id, word_frequency). \n",
    "For example: (0,1)= word_id 0 occurs once in the first document. These are the input labels for the LDA model.\n",
    "Corpus is a simple set of documents. These are the training labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c0cb22-843c-4601-832b-cd3d5dd33717",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dictionary \n",
    "id2word = corpora.Dictionary(data_lem)\n",
    "\n",
    "#Create corpus \n",
    "keywords = data_lem\n",
    "\n",
    "# Term Document Frequency \n",
    "corpus = [id2word.doc2bow(keyword) for keyword in keywords]\n",
    "\n",
    "#View \n",
    "print(corpus[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0214086-b91e-40a7-928b-d54d9606a7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#seeing what word a given id corresponds to, passing the id as a key to the dictionary\n",
    "id2word[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e421ad-dbe9-42cc-89cb-8b5a106b9c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#printing corpus (term-frequency)\n",
    "[[(id2word[id], freq) for id,freq in cp] for cp in corpus[:2]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5312f11a-017a-4db9-9dc8-03ae620de87b",
   "metadata": {},
   "source": [
    "### Task 7:  Building the Topic Model\n",
    "\n",
    "Now that we have our corpus and dictionary, all we need is to provide the number of topics as well. \n",
    "\n",
    "**Finding the optimal number of topics for LDA**\n",
    "The approach would be to build many LDA models with different values of number of topics (k) and pick the one that gives the highest coherence value. The function `compute_coherence_values()` trains multiple LDA models and provides the models and their corresponding coherence score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183ec710-f7b1-4f85-ad5c-065f185a90bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gensim.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9393bc5d-c396-4f96-8bda-c19ebd89ec7e",
   "metadata": {},
   "source": [
    "https://stackoverflow.com/questions/32313062/what-is-the-best-way-to-obtain-the-optimal-number-of-topics-for-a-lda-model-usin "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd637319-370f-4168-b016-cf176bc07f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Considering 1-15 topics, as the last is cut off\n",
    "num_topics = list(range(14)[1:])\n",
    "num_keywords = 13\n",
    "\n",
    "LDA_models = {}\n",
    "LDA_topics = {}\n",
    "for i in num_topics:\n",
    "    LDA_models[i] = LdaModel(corpus=corpus,\n",
    "                             id2word=id2word,\n",
    "                             num_topics=i,\n",
    "                             update_every=1,\n",
    "                             chunksize=len(corpus),\n",
    "                             passes=20,\n",
    "                             alpha='auto',\n",
    "                             random_state=42)\n",
    "\n",
    "    shown_topics = LDA_models[i].show_topics(num_topics=i, \n",
    "                                             num_words=num_keywords,\n",
    "                                             formatted=False)\n",
    "    LDA_topics[i] = [[word[0] for word in topic[1]] for topic in shown_topics]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f99e6b7-14cc-4108-9593-b725b9582b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity(topic_1, topic_2):\n",
    "    \"\"\"\n",
    "    Derives the Jaccard similarity of two topics\n",
    "\n",
    "    Jaccard similarity:\n",
    "    - A statistic used for comparing the similarity and diversity of sample sets\n",
    "    - J(A,B) = (A ∩ B)/(A ∪ B)\n",
    "    - Goal is low Jaccard scores for coverage of the diverse elements\n",
    "    \"\"\"\n",
    "    intersection = set(topic_1).intersection(set(topic_2))\n",
    "    union = set(topic_1).union(set(topic_2))\n",
    "                    \n",
    "    return float(len(intersection))/float(len(union))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287ff0a2-5874-48eb-92f2-a410a1d3fb87",
   "metadata": {},
   "outputs": [],
   "source": [
    "LDA_stability = {}\n",
    "for i in range(0, len(num_topics)-1):\n",
    "    jaccard_sims = []\n",
    "    for t1, topic1 in enumerate(LDA_topics[num_topics[i]]): # pylint: disable=unused-variable\n",
    "        sims = []\n",
    "        for t2, topic2 in enumerate(LDA_topics[num_topics[i+1]]): # pylint: disable=unused-variable\n",
    "            sims.append(jaccard_similarity(topic1, topic2))    \n",
    "        \n",
    "        jaccard_sims.append(sims)    \n",
    "    \n",
    "    LDA_stability[num_topics[i]] = jaccard_sims\n",
    "                \n",
    "mean_stabilities = [np.array(LDA_stability[i]).mean() for i in num_topics[:-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90133ef8-6faf-4d75-ae8c-7e217b817c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "coherences = [CoherenceModel(model=LDA_models[i], texts=data_lem, dictionary=id2word, coherence='c_v').get_coherence()\\\n",
    "              for i in num_topics[:-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da0ee41-ca0d-4f29-8108-ad02f52194a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "coh_sta_diffs = [coherences[i] - mean_stabilities[i] for i in range(num_keywords)[:-1]] # limit topic numbers to the number of keywords\n",
    "coh_sta_max = max(coh_sta_diffs)\n",
    "coh_sta_max_idxs = [i for i, j in enumerate(coh_sta_diffs) if j == coh_sta_max]\n",
    "ideal_topic_num_index = coh_sta_max_idxs[0] # choose less topics in case there's more than one max\n",
    "ideal_topic_num = num_topics[ideal_topic_num_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a10104b-5313-4be0-9c8e-2d75627cf2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "coherences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9acb7f39-043c-4d06-b339-02f30dde73ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "ax = sns.lineplot(x=num_topics[:-1], y=mean_stabilities, label='Average Topic Overlap')\n",
    "ax = sns.lineplot(x=num_topics[:-1], y=coherences, label='Topic Coherence')\n",
    "\n",
    "ax.axvline(x=ideal_topic_num, label='Ideal Number of Topics', color='black')\n",
    "ax.axvspan(xmin=ideal_topic_num - 1, xmax=ideal_topic_num + 1, alpha=0.5, facecolor='grey')\n",
    "\n",
    "y_max = max(max(mean_stabilities), max(coherences)) + (0.10 * max(max(mean_stabilities), max(coherences)))\n",
    "ax.set_ylim([0, y_max])\n",
    "ax.set_xlim([1, num_topics[-1]-1])\n",
    "\n",
    "ax.axes.set_title('Model Metrics per Number of Topics')\n",
    "ax.set_ylabel('Metric Level')\n",
    "ax.set_xlabel('Number of Topics')\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92f5ae1-ed02-417d-9748-acc8a68a2538",
   "metadata": {},
   "source": [
    "So from the above diagram, the ideal number of topics will be 11 as it will maximize coherence and minimize the topic overlap based on the Jaccard Similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de9b98a-5874-4c6b-ba5c-186b202567f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.ldamodel.LdaModel(corpus = corpus, \n",
    "                                           id2word = id2word,\n",
    "                                           num_topics = 11,\n",
    "                                           random_state = 42,\n",
    "                                           update_every=1,\n",
    "                                           chunksize = 100,\n",
    "                                           passes = 10,\n",
    "                                           alpha = 'auto',\n",
    "                                           per_word_topics = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c60065-67fc-426d-a84f-764682a3aa09",
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272a94d2-0ccc-4fef-bad1-8dcbf002ecd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Compute Model Perplexity and Coherence Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061dcdf0-3466-405f-86fd-c1b278086218",
   "metadata": {},
   "outputs": [],
   "source": [
    "#A measure of how good the model is. The lower the better\n",
    "print('\\nPerplexity: ', lda_model.log_perplexity(corpus))\n",
    "\n",
    "#Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lem, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b43605-ed4c-49f8-aac2-1efbf55ce9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize the topics\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim_models.prepare(lda_model, corpus, id2word)\n",
    "vis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4d4b74-43b8-4fb9-8f6f-595edd60fbbb",
   "metadata": {},
   "source": [
    "So, each bubble on the left-hand side are the topics. The larger the bubble, the more prevalent is that topic. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e81c91-0d0e-4e67-a774-5f48ac4d8cf2",
   "metadata": {},
   "source": [
    "### Task 8: Analysis top n keywords in each topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cbd8f29-22fc-4e94-966e-32af8bae0c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_topics = {}\n",
    "num_terms = 10 \n",
    "lambd = 0.6\n",
    "for i in range(1,11):\n",
    "    topic = vis.topic_info[vis.topic_info.Category == 'Topic'+str(i)].copy()\n",
    "    topic['relevance'] = topic['loglift']*(1-lambd)+topic['logprob']*lambd\n",
    "    all_topics['Topic '+str(i)] = topic.sort_values(by='relevance', ascending=False).Term[:num_terms].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293a8460-66a4-4b7f-a8f8-cb35aa76214a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(all_topics).T"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cpu.m80",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m80"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
