{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16a563cf-8817-46dc-9105-36e9154b692b",
   "metadata": {},
   "source": [
    "<h2 align=center> Topic Modelling with BERTopic</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32569ba-89dd-455b-bc0c-c00a34830512",
   "metadata": {},
   "source": [
    "To deal with this large amount of text, we look towards topic modeling. A technique to automatically extract meaning from documents by identifying recurrent topics.\n",
    "\n",
    "\n",
    "`BERTopic` is a topic modelling technqiue that leverages BERT embeddings and a class-based TF-IDF to create dense clusters allowing for easily interpretable topics whilst keeping important words in the topic descriptions. \n",
    "\n",
    "The BerTopic algorithm contains 3 stages:\n",
    "\n",
    "`1. Embed the textual data(documents)`\n",
    "In this step, the algorithm extracts document embeddings with BERT, or it can use any other embedding technique.\n",
    "By default, it uses the following sentence transformers\n",
    "“paraphrase-MiniLM-L6-v2”- This is an English BERT-based model trained specifically for semantic similarity tasks.\n",
    "“paraphrase-multilingual-MiniLM-L12-v2”- This is similar to the first, with one major difference is that the xlm models work for 50+ languages.\n",
    "\n",
    "`2. Cluster Documents`\n",
    "It uses UMAP to reduce the dimensionality of embeddings and the HDBSCAN technique to cluster reduced embeddings and create clusters of semantically similar documents.\n",
    "\n",
    "`3. Create a topic representation`\n",
    "The last step is to extract and reduce topics with class-based TF-IDF and then improve the coherence of words with Maximal Marginal Relevance.\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img width=\"512px\" src='https://miro.medium.com/max/700/0*CMkR9LeJvOVJ0XGG' />\n",
    "    <p style=\"text-align: center;color:gray\">Figure 1: BERT Classification Model</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca7fdf6-8548-4823-bd94-378197ec6bef",
   "metadata": {},
   "source": [
    "This project/notebook consists of several Tasks.\n",
    "\n",
    "- **[Task 1]()**: Installing all dependencies.\n",
    "- **[Task 2]()**: Importing the required libraries in the environment.\n",
    "- **[Task 3]()**: Re-usable Functions\n",
    "- **[Task 4]()**: Exploratory Data Analysis\n",
    "- **[Task 5]()**: Modelling\n",
    "- **[Task 6]()**: Number of Topics Analysis\n",
    "- **[Task 7]()**: Finding Similar Topics\n",
    "- **[Task 8]()**: Assigning new keywords to existing topics generated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ba0ecf-e251-4ca4-9ced-a6ddb577e886",
   "metadata": {},
   "source": [
    "### Task 1: Installing all the dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33f451d7-bd8d-4eaa-ae0b-c423cb0dc951",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mException:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pip/_vendor/pkg_resources/__init__.py\", line 2851, in _dep_map\n",
      "    return self.__dep_map\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pip/_vendor/pkg_resources/__init__.py\", line 2685, in __getattr__\n",
      "    raise AttributeError(attr)\n",
      "AttributeError: _DistInfoDistribution__dep_map\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pip/basecommand.py\", line 209, in main\n",
      "    status = self.run(options, args)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pip/commands/install.py\", line 310, in run\n",
      "    wb.build(autobuilding=True)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pip/wheel.py\", line 748, in build\n",
      "    self.requirement_set.prepare_files(self.finder)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pip/req/req_set.py\", line 360, in prepare_files\n",
      "    ignore_dependencies=self.ignore_dependencies))\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pip/req/req_set.py\", line 647, in _prepare_file\n",
      "    set(req_to_install.extras) - set(dist.extras)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pip/_vendor/pkg_resources/__init__.py\", line 2810, in extras\n",
      "    return [dep for dep in self._dep_map if dep]\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pip/_vendor/pkg_resources/__init__.py\", line 2853, in _dep_map\n",
      "    self.__dep_map = self._compute_dependencies()\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pip/_vendor/pkg_resources/__init__.py\", line 2886, in _compute_dependencies\n",
      "    common = frozenset(reqs_for_extra(None))\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pip/_vendor/pkg_resources/__init__.py\", line 2883, in reqs_for_extra\n",
      "    if req.marker_fn(override={'extra':extra}):\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pip/_vendor/_markerlib/markers.py\", line 113, in marker_fn\n",
      "    return eval(compiled_marker, environment)\n",
      "  File \"<environment marker>\", line 1, in <module>\n",
      "NameError: name 'platform_system' is not defined\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "### Installing all the dependencies \n",
    "!pip install bertopic[visualization] --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "211a0f62-4d9a-42a7-b4da-2461cf463c85",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied (use --upgrade to upgrade): pip==8.1.1 in /opt/conda/lib/python3.7/site-packages\n",
      "\u001b[33mYou are using pip version 8.1.1, however version 21.3.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pip==8.1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8abaeb8-a31d-44c2-8e5e-b8f618bf429b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied (use --upgrade to upgrade): numpy==1.20 in /opt/conda/lib/python3.7/site-packages\n",
      "\u001b[33mYou are using pip version 8.1.1, however version 21.3.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install numpy==1.20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e36e45ec-f12c-4598-b630-0097502893c4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied (use --upgrade to upgrade): WordCloud in /opt/conda/lib/python3.7/site-packages\n",
      "Requirement already satisfied (use --upgrade to upgrade): pillow in /opt/conda/lib/python3.7/site-packages (from WordCloud)\n",
      "Requirement already satisfied (use --upgrade to upgrade): numpy>=1.6.1 in /opt/conda/lib/python3.7/site-packages (from WordCloud)\n",
      "Requirement already satisfied (use --upgrade to upgrade): matplotlib in /opt/conda/lib/python3.7/site-packages (from WordCloud)\n",
      "Requirement already satisfied (use --upgrade to upgrade): pyparsing>=2.2.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib->WordCloud)\n",
      "Requirement already satisfied (use --upgrade to upgrade): kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib->WordCloud)\n",
      "Requirement already satisfied (use --upgrade to upgrade): python-dateutil>=2.7 in /opt/conda/lib/python3.7/site-packages (from matplotlib->WordCloud)\n",
      "Requirement already satisfied (use --upgrade to upgrade): cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib->WordCloud)\n",
      "Requirement already satisfied (use --upgrade to upgrade): six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7->matplotlib->WordCloud)\n",
      "\u001b[33mYou are using pip version 8.1.1, however version 21.3.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install WordCloud\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f21f39d-9c9d-4d6e-b45e-a897b02a78e5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied (use --upgrade to upgrade): openpyxl in /opt/conda/lib/python3.7/site-packages\n",
      "Requirement already satisfied (use --upgrade to upgrade): et-xmlfile in /opt/conda/lib/python3.7/site-packages (from openpyxl)\n",
      "\u001b[33mYou are using pip version 8.1.1, however version 21.3.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install openpyxl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb8677e-a10d-4176-a817-1cfef7b4639a",
   "metadata": {},
   "source": [
    "### Task 2: Importing the necessary libraries in the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7da1d5c9-dc11-4072-9520-4e101d7667a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-14 09:17:41.058079: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory\n",
      "2021-12-14 09:17:41.058218: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Importing Libraries\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import openpyxl\n",
    "from copy import deepcopy\n",
    "from bertopic import BERTopic\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import plotly as py\n",
    "import plotly.graph_objs as go\n",
    "import ipywidgets as widgets\n",
    "from scipy import special\n",
    "import plotly.express as px\n",
    "\n",
    "py.offline.init_notebook_mode(connected = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0fa7a60-2fe9-408e-a011-e9c95ffa8660",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('df.csv')\n",
    "df = df.rename({'SubSegment':'Topic'}, axis = 1)\n",
    "df[['Topic', 'col2']] = df['Topic'].str.split(' - ', expand=True)\n",
    "df['col2'] = df['col2'].str.replace('col2','col')\n",
    "df = df.rename_axis('Index').reset_index()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5521917-51e1-40df-9e31-fb6dd9b5663d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Task 3: Re-usable Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99fc9e3c-01fe-4ab3-9172-25475887144e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_val(modelname, topics):\n",
    "    \"\"\"\n",
    "    Input: a) modelname: Name of the BERTopic() model used. \n",
    "           b) topics: The list of topics generated by the model.\n",
    "           \n",
    "    Function: Takes in the input and returns a dictionary with topic names as the keys and the keyword's index values from the df as the values.\n",
    "    \"\"\"\n",
    "    \n",
    "    grouped_topics = {topic: [] for topic in set(topics)}\n",
    "    \n",
    "    for index, topic in enumerate(topics):\n",
    "        grouped_topics[topic].append(index)\n",
    "        \n",
    "    return grouped_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a712f94-d7d9-42bd-9268-9995e0b34930",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_result_df(dictionary, topicsdict):\n",
    "    ''''\n",
    "    Input: a) Dictionary: The dictionary with results of dict of get_topic_val function. It has all the topics as the keys with their respective keywords as the index given by the model.\n",
    "           b) topicsdict: The dictionary with the index of the corresponding keyword row in the dataframe as the key and their string keyword as the value.\n",
    "           \n",
    "    Function: Takes in the inputs, maps the index values of keywords with their actual keyword names and returns a result dataframe.\n",
    "    '''\n",
    "    \n",
    "    key = []\n",
    "    re_keywords = []\n",
    "    val = dictionary.items()\n",
    "    \n",
    "    for i, value in val:\n",
    "        key.append(i)\n",
    "        val = [*map(topicsdict.get, value)]\n",
    "        re_keywords.append(val)\n",
    "        \n",
    "    new_dict = {k: v for k, v in zip(key, re_keywords)}\n",
    "    result_df = pd.DataFrame(new_dict.items(), columns = ['Topic Nr','Present_Input_Keywords'])\n",
    "    result_df = result_df.rename_axis('Index').reset_index()\n",
    "    \n",
    "    \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cdc4d4e7-5e2a-4d47-9fd8-1b0731f3977a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def representativedocs(model, topics, docs, keywords):\n",
    "    \"\"\"\n",
    "    Input: a) Model: Name of the model you want the results for.\n",
    "           b) topics: topics extracted by the model\n",
    "           c) docs: documents given as the input to the model. This is the different topic names that the model suggests. (Top n)\n",
    "           d) Keywords: the input keywords given to the model\n",
    "    \n",
    "    Function: Takes in all the inputs and extracts the representative documents per topic.\n",
    "    \"\"\"\n",
    "    model.get_topic_info()\n",
    "    \n",
    "    #extracting the topic names/numbers \n",
    "    top_names = model.topic_names\n",
    "    top_names = pd.DataFrame(top_names.items(), columns = [topics,docs])\n",
    "    \n",
    "    #extracting representative docs for all the topics \n",
    "    rep_docs = model.representative_docs\n",
    "    rep_docs = pd.DataFrame(rep_docs.items(), columns = [topics, keywords])\n",
    "    \n",
    "    #get topics with probability \n",
    "    top_proba = model.get_topics()\n",
    "    \n",
    "    output = pd.merge(top_names, \n",
    "                rep_docs, \n",
    "                how='left', \n",
    "                left_on='topic_num', \n",
    "                right_on='topic_num')\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e34181f2-aea9-49b1-80d6-838a2be6e223",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_final_dataframe(model, representdocsdf):\n",
    "    \"\"\"\n",
    "    Inputs: a) Model: name of the model\n",
    "            b) dataframe1: This is the dataframe formed including the topics and their top n topic names for each\n",
    "            c) representdocsdf: This is the resultant dataframe of the representative docs function\n",
    "            \n",
    "            \n",
    "    Function: Returns the resultant dataframe with topic number, their top n names with c-tf-idf scores and all the keywords they contain. \n",
    "    \"\"\"\n",
    "    dataframe1 = pd.DataFrame(model.topics.items(), columns = ['Topic Nr', 'Possible Topic Names'])\n",
    "    finaldfname = pd.merge(dataframe1, representdocsdf)\n",
    "    \n",
    "    return finaldfname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 963,
   "id": "c8cf97c8-04fc-48fb-86d9-9637457164f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def get_similarity_score(model):\n",
    "    '''\n",
    "    Parameters: \n",
    "        Inputs: a) model: the model used to train your topic modelling\n",
    "                b) topicnr: the topic for which you want to see the similarity score. IMP: here the nr is the index of the row and not the topic nr so for topic -1 = topicnr is 0\n",
    "                c) resultdf: the resultant df to merge to get combined results \n",
    "                d) threshold: the threshold above which you want to get similar topics\n",
    "        Ouput: A pandas dataframe with topicnr, topic names, keywords present (input) and the distance score for each. \n",
    "    '''\n",
    "    \n",
    "    topics = sorted(list(model.get_topics().keys()))\n",
    "\n",
    "    # Extract topic words and their frequencies\n",
    "    topic_list = sorted(topics)\n",
    "    \n",
    "    embeddings = model.c_tf_idf\n",
    "    distance_matrix = cosine_similarity(embeddings)\n",
    "    \n",
    "    most_similar_ind = []\n",
    "    most_similar_val = [] \n",
    "\n",
    "    for topic in topic_list:\n",
    "        data = distance_matrix[topic] #topic -1\n",
    "        i = np.argsort(data, axis=0)[-2] \n",
    "        most_similar_ind.append(i)   #ensure length and order for the list \n",
    "        most_similar_val.append(data[i])\n",
    "                 \n",
    "    similar_df = pd.DataFrame()\n",
    "    similar_df['Topic Nr'] = topic_list\n",
    "    similar_df['most_similar'] =  most_similar_ind\n",
    "    similar_df['similarity_score'] = most_similar_val\n",
    "    return similar_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c892a71f-31cd-412d-83d5-e1ad0d0b1d4f",
   "metadata": {},
   "source": [
    "### Task 4: Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5530eb-a115-4213-bda1-8255b50ca98f",
   "metadata": {},
   "source": [
    "`Long-tail keywords` are unpopular keyword phrases with low search volume and high variation. In other words, these queries are only searched a few times per month because they are very specific keywords, or because people phrase their queries in many different ways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6175c6e-1536-4874-90d4-51c08d4e375a",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Looking for the long-tail keywords in our dataframe\n",
    "tailnr = 17\n",
    "df[df['Keyword'].apply(lambda x: len(x)>tailnr)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931e0915-9e99-4541-8fd5-35bb85781458",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud2 = WordCloud().generate(' '.join(df['Keyword']))\n",
    "plt.figure(figsize = (10, 8), facecolor = None)\n",
    "plt.imshow(wordcloud2)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5613477b-bcd1-4e08-bebc-da350de30849",
   "metadata": {},
   "source": [
    "Analysis from the wordcloud of the keywords: \n",
    "\n",
    "1) Top three keywords in our df are `a`,`b`, and `c`.\n",
    "2) We can see different types of styles: type1, type2, type3, type 4 ....\n",
    "3) We also see most people in this dataset search for branded things that is of name. \n",
    "4) We have one location mentioned: location. \n",
    "5) We see people searching for comparing keywords for different features related to topic and topic. \n",
    "6) Some also want to learn how to use a specific functionality of topic, so name. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc365cf3-9225-490f-ae76-edb7717278ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(df,x='Topic')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ce8c63-31b9-4d99-bd25-dfb34093fa43",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(df,x='col2')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8a7fd0-65f2-4aba-8a54-6c57edca495c",
   "metadata": {},
   "source": [
    "#### Checking the topics with less count what keywords they contain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd14e4f-6979-4073-b713-c4133d92fee0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df[df['Topic']=='topicname']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8105f83b-d035-464c-a244-6dd6d0f04424",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df[df['Topic']=='topicname']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15300dec-d9f5-436a-beeb-8492b660e48b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df[df['Topic']=='topicname']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08541b9-1379-4cce-be4d-bc0b3a2cda29",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df[df['Topic']=='topicname']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9bbeb6-f774-4578-9e6e-39bf19bdbc8c",
   "metadata": {},
   "source": [
    "### Task 5: Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cae2bd02-9c73-4b75-8e2f-83b8fa8d723d",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = list(df.loc[:,'Keyword'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e381f2-41aa-4a92-b5b9-dcc3c0b7e85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b70dc2e-fb82-4bb0-b55c-2b9e164228f7",
   "metadata": {},
   "source": [
    "### Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1933563-105c-4c12-a01b-e2f6d8805447",
   "metadata": {},
   "source": [
    "#### Sentence Transformers\n",
    "are the SOTA technique for sentence, text and image embeddings. \n",
    "* Useful for semantic similarity \n",
    "* Semantic Search \n",
    "* Paraphrasing Mining\n",
    "\n",
    "We can select different sentence transformers embedding models from: https://www.sbert.net/docs/pretrained_models.html "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "58c9e507-9e3c-45e9-bbb5-05fcbab4f50f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c63710ab79954aaeb75ddd3efe6f5de0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-14 09:18:48,861 - BERTopic - Transformed documents to Embeddings\n",
      "2021-12-14 09:19:03,347 - BERTopic - Reduced dimensionality with UMAP\n",
      "2021-12-14 09:19:03,757 - BERTopic - Clustered UMAP embeddings with HDBSCAN\n",
      "2021-12-14 09:20:48,974 - BERTopic - Reduced number of topics from 62 to 33\n"
     ]
    }
   ],
   "source": [
    "sent_topic_model = BERTopic(embedding_model=\"xlm-r-bert-base-nli-stsb-mean-tokens\",calculate_probabilities=True,nr_topics='auto',verbose=True,n_gram_range=(2, 3))\n",
    "topics, probs = sent_topic_model.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8df241e-034c-4348-b842-e630fd7d959f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_topic_model.get_topic_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ec2bbbee-06fc-468a-849a-4958044935ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_count = sent_topic_model.get_topic_freq()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c936267b-1284-4549-b3da-50128c009e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_count.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878e301f-38f0-4506-bdd8-5ac0d53f9fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.bar(topic_count,x='Topic',y='Count', title = 'Distribution of Topic Generated')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf915eb0-08f9-42d7-ba95-559a5cc134b0",
   "metadata": {},
   "source": [
    "### Task 6: Number of Topic Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5a011e-1aad-4cdb-90be-13f216bcedc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_similar_dict = dict(zip(df.Index, df.Keyword))\n",
    "grouped_topics = get_topic_val(sent_topic_model, topics)\n",
    "res_df = make_result_df(grouped_topics,most_similar_dict)\n",
    "result_df = make_final_dataframe(sent_topic_model,res_df)\n",
    "#result_df['count'] = result_df['Present_Input_Keywords']\n",
    "result_df['count'] = result_df['Present_Input_Keywords'].apply(lambda x: len(x)) \n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3bac448-20fb-4084-a8e0-df08b8834842",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df['count'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab209f81-058d-42e5-8e26-f9e9a8327fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "138e2ca9-3f30-4161-a678-1f2bfaae4295",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Collecting chart-studio\n",
      "  Downloading https://files.pythonhosted.org/packages/ca/ce/330794a6b6ca4b9182c38fc69dd2a9cbff60fd49421cb8648ee5fee352dc/chart_studio-1.1.0-py3-none-any.whl (64kB)\n",
      "\u001b[K    100% |████████████████████████████████| 71kB 5.6MB/s ta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied (use --upgrade to upgrade): retrying>=1.3.3 in /opt/conda/lib/python3.7/site-packages (from chart-studio)\n",
      "Requirement already satisfied (use --upgrade to upgrade): plotly in /opt/conda/lib/python3.7/site-packages (from chart-studio)\n",
      "Requirement already satisfied (use --upgrade to upgrade): requests in /opt/conda/lib/python3.7/site-packages (from chart-studio)\n",
      "Requirement already satisfied (use --upgrade to upgrade): six in /opt/conda/lib/python3.7/site-packages (from chart-studio)\n",
      "Requirement already satisfied (use --upgrade to upgrade): certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->chart-studio)\n",
      "Requirement already satisfied (use --upgrade to upgrade): urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->chart-studio)\n",
      "Requirement already satisfied (use --upgrade to upgrade): chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->chart-studio)\n",
      "Requirement already satisfied (use --upgrade to upgrade): idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->chart-studio)\n",
      "Installing collected packages: chart-studio\n",
      "Successfully installed chart-studio-1.1.0\n",
      "\u001b[33mYou are using pip version 8.1.1, however version 21.3.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install chart_studio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a2192c57-7c09-48d1-bcde-f7e6e67b46a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chart_studio\n",
    "\n",
    "username = 'akshara.shukla'\n",
    "api_key = 'apikey'\n",
    "\n",
    "chart_studio.tools.set_credentials_file(username=username, api_key = api_key)\n",
    "\n",
    "import chart_studio.plotly as py \n",
    "import chart_studio.tools as tls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d68ba8-1638-42ae-bce9-e00bc55c9aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = sent_topic_model.visualize_topics()\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e539a401-dbdb-47fe-b681-192fac803abf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://plotly.com/~akshara.shukla/1/'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "py.plot(fig, filename = 'Intertopic Distance Map df', auto_open = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c17734e-60e6-424c-b966-5ffe2784927f",
   "metadata": {},
   "source": [
    "<b>get_topics()</b> Return top n words for a specific topic and their c-TF-IDF scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508e8dbe-d399-4b65-9afd-68d4656bc560",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_topic_model.get_topic(16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38759c23-42d0-40db-a5e9-9f7324f79333",
   "metadata": {},
   "source": [
    "Having generated topic embeddings, through both c-TF-IDF and embeddings, we can create a similarity matrix by simply applying cosine similarities through those topic embeddings. The result will be a matrix indicating how similar certain topics are to each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4382a8-4c9f-4999-af22-70e955c7dfdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_topic_model.visualize_heatmap(n_clusters=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a34049-dd16-4668-9527-630c96bf5b8d",
   "metadata": {},
   "source": [
    "##### Combining documents with similarity score higher than 0.70\n",
    "1. Document -1 = 1(0.89), 2 (0.74), 6 (0.84), 11 (0.78), 13 (0.75),22 (0.74)\n",
    "2. Document 1 = 22 (0.80), 13 (0.77), 11 (0.81), 6 (0.80), -1 (0.89) \n",
    "3. Document 6 \n",
    "1. 11 and 14 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26c9e86-d544-4b8a-9482-774f5d200a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_topic_model.get_topic(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58bb9423-eae4-481e-b31f-8fe823ea7657",
   "metadata": {},
   "source": [
    "We can visualize the selected terms for a few topics by creating bar charts out of the c-TF-IDF scores for each topic representation. Insights can be gained from the relative c-TF-IDF scores between and within topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc86542-33d0-49ba-ae21-6c89cf789f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_topic_model.visualize_barchart(topics = [1,2,3,4,5,6,7])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a049374-e9c9-43ff-a9e9-ff58149fc15d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Task 7: Finding Similar Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6c737a-fa49-4589-8697-8a10a1874c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_similarity_score(sent_topic_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dee30ff-af83-4712-91ab-4fe3b76dd858",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_topic_model.get_topic(13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f715099f-fbfa-4283-89b1-d0e857799bc8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sent_topic_model.get_topic(13)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1bac666-4f0e-4c61-90cb-08aa50f18608",
   "metadata": {},
   "source": [
    "### Task 8: Assigning new keywords to existing topics generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42d8d17-37a9-4be2-856c-eda2733b39ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_topics, similarity = sent_topic_model.find_topics(\"cheap binoculars\", top_n=5); \n",
    "print(similar_topics)\n",
    "print(similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022d8fda-d10d-4b33-b764-064746da3faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_topic_model.find_topics(\"topicnr\")"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cpu.m80",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m80"
  },
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
